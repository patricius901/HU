{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173c6dba-fe81-42f0-a832-ff51a6449d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /opt/conda/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (3.11.16)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (2025.3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (3.1.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (7.0.0)\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (3.2.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.12/site-packages (from torch_geometric) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch_geometric) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch_geometric) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch_geometric) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch_geometric) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.12/site-packages (from aiohttp->torch_geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch_geometric) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->torch_geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->torch_geometric) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch_geometric --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340a686a-da57-4e0e-909b-8cf6fc915392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f060e5-6726-403b-b4a8-4419d8c1b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading path/to/SoccerNet/tracking-2023/train.zip...: : 9.58GiB [04:49, 33.0MiB/s]                          \n",
      "Downloading path/to/SoccerNet/tracking-2023/test.zip...: : 8.71GiB [05:05, 28.5MiB/s]                          \n",
      "Downloading path/to/SoccerNet/tracking-2023/challenge2023.zip...: : 5.31GiB [03:12, 27.6MiB/s]                          \n"
     ]
    }
   ],
   "source": [
    "from SoccerNet.Downloader import SoccerNetDownloader\n",
    "mySoccerNetDownloader = SoccerNetDownloader(LocalDirectory=\"path/to/SoccerNet\")\n",
    "#mySoccerNetDownloader.downloadDataTask(task=\"tracking\", split=[\"train\",\"test\",\"challenge\"])\n",
    "mySoccerNetDownloader.downloadDataTask(task=\"tracking-2023\", split=[\"train\", \"test\", \"challenge\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4fc5627-c7bc-4a15-8487-3be5f5f0719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to one sequence\n",
    "base_path = Path(\"path/to/SoccerNet/tracking-2023/train/SNMOT-060\")\n",
    "\n",
    "# Load ground truth\n",
    "gt_path = base_path / \"gt\" / \"gt.txt\"\n",
    "gt = pd.read_csv(gt_path, header=None)\n",
    "gt.columns = [\"frame\", \"id\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"class_id\", \"visibility\",\"extra\"]\n",
    "\n",
    "# Load detections\n",
    "det_path = base_path / \"det\" / \"det.txt\"\n",
    "det = pd.read_csv(det_path, header=None)\n",
    "det.columns = [\"frame\", \"id\", \"x\", \"y\", \"w\", \"h\", \"conf\", \"class_id\", \"visibility\",\"extra\"]\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.optionxform = str  # preserve case of keys\n",
    "config.read(base_path / \"gameinfo.ini\")\n",
    "\n",
    "seq = configparser.ConfigParser()\n",
    "config.optionxform = str  # preserve case of keys\n",
    "seq.read(base_path / \"seqinfo.ini\")\n",
    "\n",
    "ball_id = None\n",
    "players_left = []\n",
    "players_right = []\n",
    "referees = []\n",
    "goalkeepers_left = []\n",
    "goalkeepers_right = []\n",
    "\n",
    "for key, value in config[\"Sequence\"].items():\n",
    "    if key.startswith(\"trackletID_\"):\n",
    "        tid = int(key.replace(\"trackletID_\", \"\"))\n",
    "\n",
    "        role, role_info = value.split(\";\")\n",
    "\n",
    "        # Ball\n",
    "        if role == \"ball\":\n",
    "            ball_id = tid\n",
    "\n",
    "        # Player – two teams\n",
    "        elif role == \"player team left\":\n",
    "            players_left.append(tid)\n",
    "        elif role == \"player team right\":\n",
    "            players_right.append(tid)\n",
    "\n",
    "        # Goalkeepers\n",
    "        elif role == \"goalkeepers team left\" or role == \"goalkeeper team left\":\n",
    "            goalkeepers_left.append(tid)\n",
    "        elif role == \"goalkeepers team right\" or role == \"goalkeeper team right\":\n",
    "            goalkeepers_right.append(tid)\n",
    "\n",
    "        # Referees\n",
    "        elif role.startswith(\"referee\"):\n",
    "            referees.append(tid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "943d18de-6ac2-4a7a-80d9-290fc43a4c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ball ID: 18\n",
      "Left team players: [1, 2, 11, 12, 13, 15, 16, 19, 20, 21]\n",
      "Right team players: [3, 4, 5, 6, 7, 8, 9, 10, 23, 24]\n",
      "Left GK: [22]\n",
      "Right GK: [25]\n",
      "Referees: [14, 17, 26]\n"
     ]
    }
   ],
   "source": [
    "print(\"Ball ID:\", ball_id)\n",
    "print(\"Left team players:\", players_left)\n",
    "print(\"Right team players:\", players_right)\n",
    "print(\"Left GK:\", goalkeepers_left)\n",
    "print(\"Right GK:\", goalkeepers_right)\n",
    "print(\"Referees:\", referees)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e111b2f-e598-45cd-b5b6-fb1eecb02d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seqs in train: 100%|██████████| 57/57 [03:48<00:00,  4.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 42750 graphs to outputs/graphs/train_graphs.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seqs in test: 100%|██████████| 49/49 [03:14<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 36750 graphs to outputs/graphs/test_graphs.pt\n",
      "Split not found, skipping: path/to/SoccerNet/tracking-2023/challenge\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GNN preprocessing pipeline for SoccerNet-tracking (tracking-2023 style)\n",
    "Produces per-frame graphs (torch_geometric.data.Data) and saves them per split.\n",
    "\n",
    "Expected folder layout for each sequence:\n",
    "  <BASE_DIR>/<split>/<SEQUENCE_NAME>/\n",
    "      gt/gt.txt\n",
    "      det/det.txt\n",
    "      gameinfo.ini\n",
    "      seqinfo.ini\n",
    "\n",
    "Outputs:\n",
    "  outputs/graphs/<split>_graphs.pt  # list of Data objects (one per frame)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "BASE_DIR = Path(\"path/to/SoccerNet/tracking-2023\")  # change to your root\n",
    "SPLITS = [\"train\", \"test\", \"challenge\"]\n",
    "OUTPUT_DIR = Path(\"outputs/graphs\")\n",
    "K_NEIGHBORS = 6            # k for k-NN graph edges (excluding self loops)\n",
    "USE_TEMPORAL_EDGES = False # set True to link same tracklet across consecutive frames\n",
    "TEMPORAL_WINDOW = 1        # how many future frames to link (1 => t -> t+1)\n",
    "INCLUDE_REFS = False       # include referees as nodes? usually False\n",
    "DEVICE = \"cpu\"\n",
    "# -----------------------------\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS: parsers\n",
    "# -----------------------------\n",
    "def read_seqinfo(seqinfo_path: Path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.optionxform = str\n",
    "    config.read(seqinfo_path)\n",
    "    s = config[\"Sequence\"]\n",
    "    frame_rate = int(s.get(\"frameRate\", 25))\n",
    "    seq_length = int(s.get(\"seqLength\", 0))\n",
    "    im_w = int(s.get(\"imWidth\", 1920))\n",
    "    im_h = int(s.get(\"imHeight\", 1080))\n",
    "    return {\"frame_rate\": frame_rate, \"seq_length\": seq_length, \"im_w\": im_w, \"im_h\": im_h}\n",
    "\n",
    "def read_gameinfo(gameinfo_path: Path):\n",
    "    config = configparser.ConfigParser()\n",
    "    config.optionxform = str\n",
    "    config.read(gameinfo_path)\n",
    "    seq = config[\"Sequence\"]\n",
    "    # parse trackletID_* entries\n",
    "    mapping = {}  # tid -> role_info (role, id_string)\n",
    "    for key, value in seq.items():\n",
    "        if key.startswith(\"trackletID_\"):\n",
    "            tid = int(key.replace(\"trackletID_\", \"\"))\n",
    "            # value like \" player team left;10\"\n",
    "            try:\n",
    "                role, role_info = value.split(\";\", 1)\n",
    "                role = role.strip()\n",
    "                role_info = role_info.strip()\n",
    "            except ValueError:\n",
    "                role = value.strip()\n",
    "                role_info = \"\"\n",
    "            mapping[tid] = {\"role\": role, \"role_info\": role_info}\n",
    "    # also get actionClass/actionPosition if present\n",
    "    action_class = seq.get(\"actionClass\", None)\n",
    "    action_position = seq.get(\"actionPosition\", None)\n",
    "    return {\"mapping\": mapping, \"action_class\": action_class, \"action_position\": action_position}\n",
    "\n",
    "def load_gt(gt_path: Path):\n",
    "    # gt.txt format: frame,trackId,x,y,w,h,conf,unused,unused,unused\n",
    "    df = pd.read_csv(gt_path, header=None)\n",
    "    # ensure at least 7 columns; sometimes files have 10 columns MOT-like\n",
    "    if df.shape[1] >= 7:\n",
    "        cols = df.shape[1]\n",
    "        # create generic names for first 7 and extras\n",
    "        names = [\"frame\",\"track_id\",\"x\",\"y\",\"w\",\"h\",\"conf\"] + [f\"c{i}\" for i in range(8, cols+1)]\n",
    "        df.columns = names[:df.shape[1]]\n",
    "    else:\n",
    "        df.columns = [\"frame\",\"track_id\",\"x\",\"y\",\"w\",\"h\",\"conf\"]\n",
    "    # types\n",
    "    df[\"frame\"] = df[\"frame\"].astype(int)\n",
    "    df[\"track_id\"] = df[\"track_id\"].astype(int)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Graph builder: per-frame\n",
    "# -----------------------------\n",
    "def build_graph_for_frame(nodes_df, roles_map, seq_info, k_neighbors=6, include_refs=False):\n",
    "    \"\"\"\n",
    "    nodes_df: DataFrame with rows for tracklets present in this frame:\n",
    "      columns: ['track_id','x','y','w','h','conf', ...]\n",
    "    roles_map: dict track_id -> {'role':..., 'role_info':...}\n",
    "    seq_info: {'im_w','im_h',...}\n",
    "    \"\"\"\n",
    "    # if empty frame\n",
    "    if nodes_df.shape[0] == 0:\n",
    "        return None\n",
    "\n",
    "    # normalize coordinates to [0,1]\n",
    "    im_w, im_h = seq_info[\"im_w\"], seq_info[\"im_h\"]\n",
    "    xs = nodes_df[\"x\"].to_numpy().astype(float) / im_w\n",
    "    ys = nodes_df[\"y\"].to_numpy().astype(float) / im_h\n",
    "    ws = nodes_df[\"w\"].to_numpy().astype(float) / im_w\n",
    "    hs = nodes_df[\"h\"].to_numpy().astype(float) / im_h\n",
    "    confs = nodes_df[\"conf\"].to_numpy().astype(float)\n",
    "\n",
    "    # role one-hot: player_left, player_right, goalkeeper_left, goalkeeper_right, ball, referee, other\n",
    "    role_vectors = []\n",
    "    track_ids = nodes_df[\"track_id\"].to_numpy().astype(int).tolist()\n",
    "    for tid in track_ids:\n",
    "        r = roles_map.get(tid, {}).get(\"role\", \"\").lower()\n",
    "        vec = [\n",
    "            1 if r == \"player team left\" else 0,\n",
    "            1 if r == \"player team right\" else 0,\n",
    "            1 if \"goalkeep\" in r else 0,\n",
    "            1 if r == \"ball\" else 0,\n",
    "            1 if r.startswith(\"referee\") else 0,\n",
    "            1 if r == \"\" else 0\n",
    "        ]\n",
    "        role_vectors.append(vec)\n",
    "    role_vectors = np.array(role_vectors, dtype=float)\n",
    "\n",
    "    # node features: [x,y,w,h,conf, role_onehot...]\n",
    "    node_features = np.concatenate([\n",
    "        xs.reshape(-1,1),\n",
    "        ys.reshape(-1,1),\n",
    "        ws.reshape(-1,1),\n",
    "        hs.reshape(-1,1),\n",
    "        confs.reshape(-1,1),\n",
    "        role_vectors\n",
    "    ], axis=1)\n",
    "\n",
    "    # create k-NN edges\n",
    "    # if nodes < k+1, connect fully\n",
    "    N = node_features.shape[0]\n",
    "    if N <= 1:\n",
    "        edge_index = np.zeros((2,0), dtype=int)\n",
    "    else:\n",
    "        if N <= k_neighbors:\n",
    "            # full directed edges excluding self-loops\n",
    "            sources = []\n",
    "            targets = []\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    if i != j:\n",
    "                        sources.append(i); targets.append(j)\n",
    "            edge_index = np.vstack([sources, targets])\n",
    "        else:\n",
    "            coords = np.vstack([xs, ys]).T\n",
    "            nbrs = NearestNeighbors(n_neighbors=k_neighbors+1, algorithm=\"auto\").fit(coords)\n",
    "            distances, indices = nbrs.kneighbors(coords)\n",
    "            # indices includes self at pos 0; skip it\n",
    "            sources = []\n",
    "            targets = []\n",
    "            for i in range(N):\n",
    "                for nb in indices[i,1:]:\n",
    "                    sources.append(i)\n",
    "                    targets.append(int(nb))\n",
    "            edge_index = np.vstack([sources, targets])\n",
    "\n",
    "    # convert to torch\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long) if edge_index.size else torch.empty((2,0), dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"x\": x,\n",
    "        \"edge_index\": edge_index,\n",
    "        \"track_ids\": track_ids,\n",
    "        \"raw_coords\": np.vstack([xs, ys]).T\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop: build dataset for split\n",
    "# -----------------------------\n",
    "def process_split(split_dir: Path, out_path: Path, k_neighbors=6, include_refs=False):\n",
    "    \"\"\"\n",
    "    Processes all sequences in split_dir and appends per-frame Data objects to list.\n",
    "    Saves list to out_path.\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    sequences = [p for p in split_dir.iterdir() if p.is_dir()]\n",
    "    sequences = sorted(sequences)\n",
    "\n",
    "    for seq_path in tqdm.tqdm(sequences, desc=f\"Seqs in {split_dir.name}\"):\n",
    "        gt_path = seq_path / \"gt\" / \"gt.txt\"\n",
    "        gameinfo_path = seq_path / \"gameinfo.ini\"\n",
    "        seqinfo_path = seq_path / \"seqinfo.ini\"\n",
    "        events_path = seq_path / \"events.json\"  # optional\n",
    "\n",
    "        if not gt_path.exists() or not gameinfo_path.exists() or not seqinfo_path.exists():\n",
    "            # skip if missing\n",
    "            print(\"Skipping (missing files):\", seq_path)\n",
    "            continue\n",
    "\n",
    "        seq_info = read_seqinfo(seqinfo_path)\n",
    "        game_info = read_gameinfo(gameinfo_path)\n",
    "        roles_map = game_info[\"mapping\"]\n",
    "\n",
    "        gt = load_gt(gt_path)\n",
    "\n",
    "        # precompute velocity: group by track_id sorted by frame\n",
    "        gt_sorted = gt.sort_values([\"track_id\",\"frame\"])\n",
    "        # compute per-row vx,vy using frame diffs (simple finite diff)\n",
    "        gt_sorted[\"vx\"] = 0.0\n",
    "        gt_sorted[\"vy\"] = 0.0\n",
    "        for tid, group in gt_sorted.groupby(\"track_id\"):\n",
    "            frames = group[\"frame\"].values\n",
    "            xs = group[\"x\"].values\n",
    "            ys = group[\"y\"].values\n",
    "            # forward diff\n",
    "            if len(frames) >= 2:\n",
    "                dx = np.diff(xs) / np.maximum(1, np.diff(frames))  # pixels/frame\n",
    "                dy = np.diff(ys) / np.maximum(1, np.diff(frames))\n",
    "                # assign to second row onwards\n",
    "                gt_sorted.loc[group.index[1:], \"vx\"] = dx\n",
    "                gt_sorted.loc[group.index[1:], \"vy\"] = dy\n",
    "\n",
    "        # create mapping frame -> rows\n",
    "        frames = sorted(gt[\"frame\"].unique())\n",
    "        # optional events mapping\n",
    "        events_map = load_events(events_path)\n",
    "\n",
    "        # iterate frames\n",
    "        for fr in frames:\n",
    "            nodes_df = gt[gt[\"frame\"] == fr].copy().reset_index(drop=True)\n",
    "            # attach vx/vy from gt_sorted\n",
    "            vxvy = gt_sorted[gt_sorted[\"frame\"] == fr][[\"track_id\",\"vx\",\"vy\"]]\n",
    "            if not vxvy.empty:\n",
    "                nodes_df = nodes_df.merge(vxvy, on=\"track_id\", how=\"left\")\n",
    "\n",
    "            # optionally filter refs if include_refs False\n",
    "            if not include_refs:\n",
    "                # keep only players and ball and keep goalkeepers too\n",
    "                keep_track_ids = [tid for tid,r in roles_map.items() if (\"player\" in r[\"role\"] or \"ball\" in r[\"role\"] or \"goalkeep\" in r[\"role\"])]\n",
    "                nodes_df = nodes_df[nodes_df[\"track_id\"].isin(keep_track_ids)].reset_index(drop=True)\n",
    "                if nodes_df.empty:\n",
    "                    continue\n",
    "\n",
    "            built = build_graph_for_frame(nodes_df, roles_map, seq_info, k_neighbors=k_neighbors, include_refs=include_refs)\n",
    "            if built is None:\n",
    "                continue\n",
    "\n",
    "            x = built[\"x\"]\n",
    "            edge_index = built[\"edge_index\"]\n",
    "\n",
    "            # label: check events_map else None\n",
    "            label = events_map.get(fr, None)\n",
    "\n",
    "            # create Data object\n",
    "            data = Data(x=x, edge_index=edge_index, y=torch.tensor([0]) if label is None else torch.tensor([1]))  # placeholder binary labeling\n",
    "            # better: store raw metadata\n",
    "            data.seq_name = seq_path.name\n",
    "            data.frame = int(fr)\n",
    "            data.track_ids = built[\"track_ids\"]\n",
    "            data.raw_coords = torch.tensor(built[\"raw_coords\"], dtype=torch.float)\n",
    "            # optional: store role mapping and seq info\n",
    "            data.roles_map = roles_map\n",
    "            data.seq_info = seq_info\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "    # save list\n",
    "    torch.save(data_list, out_path)\n",
    "    print(f\"Saved {len(data_list)} graphs to {out_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run for splits\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    for split in SPLITS:\n",
    "        split_dir = BASE_DIR / split\n",
    "        if not split_dir.exists():\n",
    "            print(\"Split not found, skipping:\", split_dir)\n",
    "            continue\n",
    "        out_path = OUTPUT_DIR / f\"{split}_graphs.pt\"\n",
    "        process_split(split_dir, out_path, k_neighbors=K_NEIGHBORS, include_refs=INCLUDE_REFS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d8f44-ea81-4248-bfa7-604f6dcdad47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
